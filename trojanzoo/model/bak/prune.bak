
    def prune(self, percent=0.1, iter_prune=35, iter_train=20,
              _global=True, rewind=True, adv_train=None, smooth=False, reinit=False,
              prefix='_prune', _continue=False, save=True, **kwargs):
        # Weight Initialization
        # self.apply(self.weight_init)
        if not _continue:
            self.make_mask()
        initial_state_dict = OrderedDict()
        for key in self.state_dict().keys():
            if 'mask' not in key:
                initial_state_dict[key] = deepcopy(self.state_dict()[key])
        if adv_train is not None:
            prefix += '_adv_' + adv_train
        if smooth:
            prefix += '_smooth'
        if _global:
            prefix += '_global'
        if rewind:
            prefix += '_rewind'
        prefix += '_percent%.3f' % percent
        for i in range(iter_prune):
            print('Prune Iteration: ', i)
            if i != 0:
                self.prune_step(percent, _global=_global)
            if reinit:
                self.apply(self.weight_init)
            elif not rewind:
                self.load_state_dict(initial_state_dict, strict=False)
            if save:
                self.save_weights(prefix=prefix + '_%d' %
                                  i, result=True, output=True)
            self.adv_train(iter_train, mode=adv_train, smooth=smooth,
                           save=False, **kwargs)

    # Prune by Percentile module
    def prune_step(self, percent=0.1, _global=True, **kwargs):
        if not _global:
            for module in self.modules():
                if 'weight_orig' in module._parameters.keys():
                    # prune.identity(module, 'weight')
                    mask = module.weight_mask
                    weight = (module.weight_mask*module.weight_orig).abs()
                    # flattened array of nonzero values
                    percentile_value = percentile(
                        weight[mask.bool()], percent*100)

                    # Convert Tensors to numpy and calculate
                    new_mask = torch.where(
                        weight < percentile_value, to_tensor([0.0]), mask)
                    # Apply new mask
                    module.weight_mask = new_mask
        else:
            W_shapes = []
            res = []

            nnz = 0
            for name, module in self.named_modules():
                if 'weight_orig' not in module._parameters.keys():
                    continue
                mask = module.weight_mask
                weight = (module.weight_mask*module.weight_orig).abs()
                total_num = mask.numel()
                valid_num = int(mask.sum())
                zero_num = total_num-valid_num + int(percent*valid_num)

                W_shapes.append((name, weight.data.shape))
                res.append(weight.data.view(-1))
                nnz += zero_num
            res = torch.cat(res, dim=0)
            _, idx = torch.topk(res, nnz, largest=False, sorted=False)

            new_res = to_tensor(torch.ones_like(
                res), dtype='float', device=res.device)
            new_res[idx] = 0.0
            # # flattened array of nonzero values
            # percentile_value = percentile(param, (1-k)*100)
            # # Convert Tensors to numpy and calculate
            # param.data = torch.where(
            #     param < percentile_value, to_tensor([0.0]), param.data)
            offset = 0
            W_shapes = iter(W_shapes)
            for name, module in self.named_modules():
                if 'weight_orig' not in module._parameters.keys():
                    continue
                name_, shape = next(W_shapes)
                assert name_ == name
                mask = module.weight_mask
                mask.data = new_res[offset:offset+mask.numel()].view(shape)
                offset += mask.numel()

    def make_mask(self):
        for module in self.modules():
            if 'weight' in module._parameters.keys():
                prune.identity(module, 'weight')

    # Function for Initialization
    @staticmethod
    def weight_init(m):
        '''
        Usage:
            model = Model()
            model.apply(weight_init)
        '''
        if isinstance(m, nn.Conv1d):
            init.normal_(m.weight.data)
            if m.bias is not None:
                init.normal_(m.bias.data)
        elif isinstance(m, nn.Conv2d):
            init.xavier_normal_(m.weight.data)
            if m.bias is not None:
                init.normal_(m.bias.data)
        elif isinstance(m, nn.Conv3d):
            init.xavier_normal_(m.weight.data)
            if m.bias is not None:
                init.normal_(m.bias.data)
        elif isinstance(m, nn.ConvTranspose1d):
            init.normal_(m.weight.data)
            if m.bias is not None:
                init.normal_(m.bias.data)
        elif isinstance(m, nn.ConvTranspose2d):
            init.xavier_normal_(m.weight.data)
            if m.bias is not None:
                init.normal_(m.bias.data)
        elif isinstance(m, nn.ConvTranspose3d):
            init.xavier_normal_(m.weight.data)
            if m.bias is not None:
                init.normal_(m.bias.data)
        elif isinstance(m, nn.BatchNorm1d):
            init.normal_(m.weight.data, mean=1, std=0.02)
            init.constant_(m.bias.data, 0)
        elif isinstance(m, nn.BatchNorm2d):
            init.normal_(m.weight.data, mean=1, std=0.02)
            init.constant_(m.bias.data, 0)
        elif isinstance(m, nn.BatchNorm3d):
            init.normal_(m.weight.data, mean=1, std=0.02)
            init.constant_(m.bias.data, 0)
        elif isinstance(m, nn.Linear):
            init.xavier_normal_(m.weight.data)
            init.normal_(m.bias.data)
        elif isinstance(m, nn.LSTM):
            for param in m.parameters():
                if len(param.shape) >= 2:
                    init.orthogonal_(param.data)
                else:
                    init.normal_(param.data)
        elif isinstance(m, nn.LSTMCell):
            for param in m.parameters():
                if len(param.shape) >= 2:
                    init.orthogonal_(param.data)
                else:
                    init.normal_(param.data)
        elif isinstance(m, nn.GRU):
            for param in m.parameters():
                if len(param.shape) >= 2:
                    init.orthogonal_(param.data)
                else:
                    init.normal_(param.data)
        elif isinstance(m, nn.GRUCell):
            for param in m.parameters():
                if len(param.shape) >= 2:
                    init.orthogonal_(param.data)
                else:
                    init.normal_(param.data)

    def prune_test(self, percent=0.1, _iter=None, iter_prune=35, iter_train=20,
                   _global=True, rewind=True, adv_train=None, smooth=False, reinit=False,
                   prefix='_prune', _continue=False, save=True, **kwargs):
        # Weight Initialization
        # self.apply(self.weight_init)
        if not _continue:
            self.make_mask()
        if adv_train is not None:
            prefix += '_adv_' + adv_train
        if smooth:
            prefix += '_smooth'
        if _global:
            prefix += '_global'
        if rewind:
            prefix += '_rewind'
        prefix += '_percent%.3f' % percent
        if _iter is None:
            for i in range(iter_prune):
                prefix = '_%d' % i
                self.load_pretrained_weights(prefix=prefix, result=True)
                if reinit:
                    self.apply(self.weight_init)
                self.adv_train(iter_train, save=False, **kwargs)
        else:
            prefix = '_%d' % _iter
            self.load_pretrained_weights(prefix=prefix, result=True)
            if reinit:
                self.apply(self.weight_init)
            self.adv_train(iter_train, save=False, **kwargs)

    def prune_atmc(self, epoch, perturb=None, percent=0.1, m=8, alpha=2.0/255, epsilon=8.0/255, iteration=20, lr=5e-3, train_opt='full', optim_type='SGD', lr_scheduler=True, validate_interval=10, save=True, prefix='_atmc', parallel=True, **kwargs):
        prefix += '_%.3f' % percent
        self.train()

        _lr_scheduler = self.define_optimizer(
            train_opt=train_opt, lr_scheduler=True, lr_step=10, **kwargs)
        optimizer = _lr_scheduler.optimizer
        optimizer.zero_grad()

        if perturb is None:
            from trojanzoo.utils.loader import get_attack
            perturb = get_attack(
                'pgd', model=self, iteration=iteration, alpha=alpha, epsilon=epsilon)

        _, best_acc, _ = self._validate()
        _, best_adv_acc, _ = self.adv_validate(perturb=perturb, targeted=False)
        # _, best_adv_acc, _ = self.adv_validate(
        #     validloader=validloader, perturb=perturb, targeted=True)
        self.train()

        losses = AverageMeter('Loss', ':.4e')
        top1 = AverageMeter('Acc@1', ':6.2f')
        top5 = AverageMeter('Acc@5', ':6.2f')

        for _epoch in range(epoch):
            losses.reset()
            top1.reset()
            top5.reset()
            for i, data in enumerate(self.dataset.loader['train']):
                # data_time.update(time.time() - end)
                _input, _label = self.get_data(data, mode='train')
                noise = to_tensor(torch.zeros_like(_input))
                adv_X = _input
                for k in range(m):
                    optimizer.zero_grad()
                    _output = self.get_logits(adv_X)
                    loss = self.criterion(_output, _label)
                    loss.backward()
                    optimizer.step()
                    optimizer.zero_grad()

                    self.project(k=percent)

                    acc1, acc5 = self.accuracy(_output, _label, topk=(1, 5))
                    losses.update(loss.item(), _label.size(0))
                    top1.update(acc1[0], _label.size(0))
                    top5.update(acc5[0], _label.size(0))

                    empty_cache()
                    adv_X, _ = perturb.perturb(_input, noise=noise, target=_label,
                                               targeted=False, iteration=1)
                    self.train()
                    self.zero_grad()
                    # batch_time.update(time.time() - end)
                    # end = time.time()

                    # if i % 10 == 0:
                    #     progress.display(i)
            print(('Epoch: [%d/%d],' % (_epoch+1, epoch)).ljust(25, ' ') +
                  'Loss: %.4f,\tTop1 Acc: %.3f,\tTop5 Acc: %.3f' % (losses.avg, top1.avg, top5.avg))
            if lr_scheduler:
                _lr_scheduler.step()

            if validate_interval != 0:
                if (_epoch+1) % validate_interval == 0 or _epoch == epoch - 1:
                    _, cur_acc, _ = self._validate()
                    _, cur_adv_acc, _ = self.adv_validate(
                        perturb=perturb, targeted=False)
                    # _, cur_adv_acc, _ = self.adv_validate(perturb=perturb, targeted=True)
                    self.train()
                    if cur_adv_acc > best_adv_acc and save:
                        self.save_weights(
                            prefix=prefix, result=True, output=True)
                        best_adv_acc = cur_adv_acc
                    print('---------------------------------------------------')
        self.zero_grad()
        self.eval()

    # Input:
    # gamma (stepsize sequence)
    # n, T (update steps)
    # pho, k, b, Delta (hyper-parameters)
    # def prune_atmc(self, epoch=150, iteration=7, rho=0.1, k=0.1, Delta=4.0/255, perturb=None, bias=False, validate_interval=10, save=True, **kwargs):
        # lr_scheduler = self.define_optimizer(
        #     train_opt='full', optim_type='SGD', lr_scheduler=True)
        # optimizer = lr_scheduler.optimizer
        # optimizer.zero_grad()

        # alpha = 1.25 * Delta / iteration
        # if perturb is None:
        #     from package.utils.loader import get_perturb
        #     perturb = get_perturb(
        #         'pgd', model=self, iteration=iteration, alpha=alpha, epsilon=Delta)

        # # # Initialize theta_prime and u
        # # u = OrderedDict()
        # # theta_prime = OrderedDict()
        # # for name, param in self.named_parameters():
        # #     theta_prime[name] = deepcopy(param)
        # #     u[name] = to_tensor(torch.zeros_like(param))

        # self.train()
        # losses = AverageMeter('Loss', ':.4e')
        # top1 = AverageMeter('Acc@1', ':6.2f')
        # top5 = AverageMeter('Acc@5', ':6.2f')
        # for _epoch in range(epoch):
        #     losses.reset()
        #     top1.reset()
        #     top5.reset()
        #     for i, data in enumerate(self.dataset.loader['train']):
        #         _input, _label = self.get_data(data, mode='valid')
        #         adv_input, _iter = perturb.perturb(_input, targeted=False)
        #         _output = self.get_logits(adv_input)
        #         loss = self.criterion(_output, _label)
        #         loss.backward()
        #         optimizer.step()
        #         optimizer.zero_grad()
        #         self.project(k=k, bias=bias)

        #         acc1, acc5 = self.accuracy(_output, _label, topk=(1, 5))
        #         losses.update(loss.item(), _label.size(0))
        #         top1.update(acc1[0], _label.size(0))
        #         top5.update(acc5[0], _label.size(0))

        #     print(('Epoch: [%d/%d],' % (_epoch+1, epoch)).ljust(25, ' ') +
        #           'Loss: %.4f,\tTop1 Acc: %.3f,\tTop5 Acc: %.3f' % (losses.avg, top1.avg, top5.avg))
        #     lr_scheduler.step()
        #     # # update theta
        #     # for name, param in self.named_parameters():
        #     #     norm_grad = param.data-theta_prime[name]+u[name]
        #     #     param.data = param.data - \
        #     #         gamma[t]*(param.grad.data+rho*norm_grad)

        #     # # update theta_prime
        #     # zero_kmeans(add_dict(self.state_dict(), u), B=2**b)
        #     # # update u
        #     # for name, param in self.named_parameters():
        #     #     u[name] = u[name]+param.data-theta_prime[name]
        #     if validate_interval != 0:
        #         if (_epoch+1) % validate_interval == 0 or _epoch == epoch - 1:
        #             _, cur_acc, _ = self._validate()
        #             _, cur_adv_acc, _ = self.adv_validate(
        #                 perturb=perturb, targeted=False)
        #             # _, cur_adv_acc, _ = self.adv_validate(perturb=perturb, targeted=True)
        #             self.train()
        #             if cur_adv_acc > best_adv_acc and save:
        #                 self.save_weights(prefix=prefix)
        #                 best_adv_acc = cur_adv_acc
        #             print('---------------------------------------------------')
        # self.zero_grad()
        # self.eval()

    def project(self, k=0.1, bias=False):
        W_shapes = []
        res = []
        for name, param in self.named_parameters():
            if bias:
                if 'weight' not in name and 'bias' not in name:
                    continue
            elif 'weight' not in name:
                continue
            W_shapes.append((name, param.data.shape))
            res.append(param.data.view(-1))
        res = torch.cat(res, dim=0)
        nnz = round(res.shape[0]*k)
        _, idx = torch.topk(res.abs(), int(
            res.shape[0]-nnz), largest=False, sorted=False)
        res[idx] = 0.0
        # # flattened array of nonzero values
        # percentile_value = percentile(param, (1-k)*100)
        # # Convert Tensors to numpy and calculate
        # param.data = torch.where(
        #     param < percentile_value, to_tensor([0.0]), param.data)
        offset = 0
        W_shapes = iter(W_shapes)
        for name, param in self.named_parameters():
            if bias:
                if 'weight' not in name and 'bias' not in name:
                    continue
            elif 'weight' not in name:
                continue
            _name, shape = next(W_shapes)
            assert _name == name
            param.data.copy_(res[offset:offset+param.numel()].view(shape))
            offset += param.numel()


# Input: U_bar (a set of real numbers), B (number of clusters)
# Output: U (quantized tensor)
# def zero_kmeans(U_bar, B):
#     # Initialize by randomly picked nonzero elements from U_bar
#     Q = [0.0]

#     return U


# def add_dict(dict1, dict2):
#     _dict = OrderedDict()
#     for key in dict1.keys():
#         _dict[key] = dict1[key]+dict2[key]
#     return _dict


# def minus_dict(dict1, dict2):
#     _dict = OrderedDict()
#     for key in dict1.keys():
#         _dict[key] = dict1[key]-dict2[key]
#     return _dict
